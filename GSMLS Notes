PRIORITIES:

Apache Kafka Notes:
https://docs.confluent.io/platform/current/installation/installing_cp/zip-tar.html
https://www.youtube.com/watch?v=heXd6JA2TQc

- I'm going to be using the community/commercial distro of Kafka: Confluent Kafka

1) Download Steps:
- Go to confluent.io/get-started
- Choose the "self-managed" deployment and download
- Choose the Community edition which is open-source and free forever
    - Download the zip file
    - Unzipped to C:\Users\Omar\Desktop\Confluent
    - set CONFLUENT_HOME=~/Confluent

2) Producer and consumer flow:
- Change the local dir and server directory in the server and zookeeper files
- Start Zookeeper
    - Start a cmd from the confluent-7.6.1 folder
    - then start the zookeeper with the following command:
        - .\bin\windows\zookeeper-server-start.bat .\etc\kafka\zookeeper.properties
        - the above command starts the zookeeper with the zookeeper config file
        - *** If you encounter the "Classpath is empty. Please build the project first e.g. by running ‘gradlew jarAll’ error"
        Use the solution in the following link: https://medium.com/@praveenkumarsingh/confluent-kafka-on-windows-how-to-fix-classpath-is-empty-cf7c31d9c787
        - *** My current client port is 2128 or 2181

- Start Kafka Server
    - Start new cmd line from the confluent-7.6.1 folder
    - then start the server with the following command:
        - .\bin\windows\kafka-server-start.bat .\etc\kafka\server.properties
        - *** My server port is 9092
- Create topics
    - Start another cmd from the confluent-7.6.1 folder
    - .\bin\windows\kafka-topics.bat --create --topic myFirstTopic --bootstrap-server localhost:9092
    - *** Be sure to set the amount of partitions I want when creating the topic
    - *** I can change the amount of partitions to the topic after creation with the following command line:
        - .\bin\windows\kafka-topics.bat --alter --topic <my_topic> --partitons <amount> --boostrap-server localhost:9092

- Start your producer
    - In the same cmd as the topic, write the following command:
        - .\bin\windows\kafka-console-producer.bat --topic myFirstTopic --bootstrap-server localhost:9092

- Start your consumer
    - Start another cmd from the confluent-7.6.1 folder
    - Start the consumer with the following command:
        *** Make sure I adjust the settings to read from latest instead of the beginning
        - .\bin\windows\kafka-console-consumer.bat --topic myFirstTopic --from-beginning --bootstrap-server localhost:9092

- Be sure to close both the Zookeeper and Server when finished



A)*** Use Apache Kafka, Apache Spark and Apache Airflow/NiFi to create a data pipeline for this system
        - Apache Kafka can help me send sales data in batches to be processed in real time instead
        of waiting for all the data to finish downloading to the process
        - Apache Airflow/NiFi will help with the workflow/data flow management of the complex system
    - Create Kafka topics called "Sold RES", "Sold MUL", "Sold LND", and "Pictures"
    - The GSMLS class will house all of the necessary functions to complete the pipeline
    - The first portion of the system will download data on a daily basis. Will be programmed using Apache Airflow and Python
        - Download the sales data and the associated pictures
    - Use pandas to convert xlsx content to JSON (json = df.to_json())
    - Produce JSON data to Kafka
    - Consume the the JSON data from Kafka using Apache Spark and complete the rest of the data transformation and enrichment
    - Store data in PostgreSQL for data exploration

1) NJTaxAssessment
*** Upload all tax municipality databases to PostgreSQL in one table
***Figure out how to properly add a package to sys.path and use it. Currently copied class from parent folder
***The url's arent https even though thats what I'm labeling them as. Figure this out. I think I need to add verifyssl=False
- Update city_database function
- Complete the code to stream Essex County property dbs (DONE)
    - The download_link isnt present sometimes and presents an error
    - Generate a different proxy then load webpage using city id (?town=0711), dont click the radio buttons anymore
    - Be sure to update the format_proxies function when done in the main file
    - May need to create a function loop to try different proxies until the download initiates
    - Use SeleniumWire to intercept some of the driver requests to read/change the payloads to get the dbs I need
- Create decorator for nj_databases to create a time constraint for downloads (1 year)
- Make sure all Essex County tax db's 'Property Location' columns are converted to string types and addresses all converted to uppercase to work with program
- Make sure the address regex patterns can recognize the S,W,E,N directions


2) GSMLS
***CHECKPOINT: sq_ft_search experiences random failures when the sidebar_buttons cant be found. Figure this out
selenium.common.exceptions.WebDriverException: Message: unknown error: failed to close window in 20 seconds
  (Session info: MicrosoftEdge=121.0.2277.128)

*** CHECKPOINT: Add the property name to the image dictionary keys
    name = soup.find('div', {'class': 'imageReportTitle'}).input.get_text() (DONE)
    - Will have to message the BS4 Object to get the name (DONE)
        - Continue figuring out how to remove the \xa0 from the strings (DONE)
    - Add the county to the xls filename. In the event an error is raised and program needs to restart, a Kafka Consumer will pull the latest
    offset to know where to resume and feed to program (DONE)
        - I need to find a way to read all partitions to find the last sent piece of data (CANT DO)
        - I need to format my logger in a way that the quarterly_sales decorator reads the file in the event of failure (DONE)
        and parses the last file name produced to know where to continue
    - Be sure to properly add the key serializer to the Kafka Producer and add the xls filename as the key (Check this)
    - Be sure to add a function to query the median sale price data for a municipality in the cleaning part of the pipeline (based on quarter)
    - Add the Kafka Control Center


**** 'xlrd' for opening old xls files works. Not sure what was going on last time but I dont have to convert to xlsx now
- add function called scrape_image_links() into download_manager() which will return a dictionary (DONE)
- Create a function which creates a metadata dict to concat necessary data to the target df. Current way of doing presents performance issues
    - Due to PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.
    Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy(
- build in latency periods to download xls file and for media page to load (DONE)
- Add kwargs args to the functions which accept the kafka producers so I can send two producers (one for pictures and one for json data) (NOT NECESSARY)
    - Just create another topic called 'testImages' and have the producer produce to each separate topic (DONE)
- Connect the Kafka Control Center to monitor my downloads
- Add try-except blocks to the kafka producers to log if the producers fail to send data to Kafka
- create function called format_data_for_kafka() which accepts the downloaded xls file name and image dict (DONE)
    - merge the latlong data from the image dict into the pandas df before publishing to Kafka (DONE)
        - df.to_json() which is an accepted Kafka datatype (DONE)
    - separate IMAGE key and json.dumps() which is an accepted Kafka datatype (DONE)
- Change publish_data_2kafka to format_data_for_kafka() and vice versa (DONE)
- Some pictures might have the same name. Account for this so images arent replaced (DONE)
- change the base paths for:
    - Selenium saved path
    - base path for publish_2kafka
    - send2trash
- have the GSMLS.results_found function return the xls file name (DONE)
- have GSMLS.download manager() return both scrape_image_link() and results_found() (DONE)
- Remove RPR from the sq_ft_finding process. Make this separate process
- Add function to all quarterly_sales functions that produce to Kafka
- Add function that uses Spark to consume from Kafka and chain together cleaning/transformation pipeline
- Delete unnecessary functions

- Find a way to clean up the code for find_sq_ft, sq_ft_search and sq_ft_search_keyerror
    convert_lot_size
  *** Organize STF folder. Remove old Excel files
_ ***Create a function called old_sold_data() that utilizes the quarterly_sales functions to download all of the listings from
    1990-2022 for RES, MUL and LND
    - I'll have to update the page_criteria function to accept another arg so I can switch between "S" and "SD" (DONE)
    - I'll have to find a way to update the quarterly_sales decorator to cycle through the old years
        - old_sold_data() doesnt need to be created. Retrofit all quarterly_sales() functions to have a status_var and year_var
        - The status_var would allow the functions switch between any property status necessary
        - For the year_var, the default value is None. If none, the decorator will use the current year time_periods dict
        - If year_var is not None, use the old_sold_time_periods dict
        - Figure out what data type will be accepted for the year_var and how to cycle through the years
        - Create an old_sold run log to house the download status of each year and qtr and shelve it(model off of GSMLS Run Dictionary)

- FileNotFoundError is raised when loading the tax_db due to certain city names not being spelled correctly (ie: Andover vs Andover Township & Borough) (DONE)
    - Incorporate FileNotFoundError exception and modify names until file found (DONE)
    - FileNotFoundError: [WinError 3] The system cannot find the path specified: 'F:\\Real Estate Investing\\JQH Holding Company LLC\\Property Data\\SUSSEX\\ANDOVER' (DONE)
    - city_list = os.listdir(target_path) (DONE)
    - Create function called tax_db_notfound(city_name2) (DONE)
    - Ocean County doesn't have tax_dbs. For cities located in these counties, set tax_db = None (DONE)
    - Have sq_ft_finder set all NA's in the mls_db to 0 and cast column datatype to int64 (DONE)

- Extend the pattern search for convert_lot_size()
    - Add a redundant .str.rstrip('ABCDEFGabcdef') on thr target1 in fix_lotsize() before trying to convert the type to float
    - Land acres pattern needs to inlcude IRR, and all other abrv (DONE)
    - Make sure the 'AC' pattern works even when 'AC' isnt there (DONE)
    - Completely redo the convert_lot_size function (DONE)
        - initial wrapper function accepts all string values
        - Then, if string value meets a certain regex pattern, it initiate the function (DONE)
        - Currently, the column is going through 2 conversion functions and giving bad results (DONE)
        - Also add to acres pattern: \.\d{1,4}\sACRE(S)?, \.\d{1,4}\sAcre(s)? (DONE)
        - Also add to lxw pattern: \d{1,5})\sX\s(\d{1,5} AVG, \d{1,3}\.\d{1,5})\sX\s(\d{1,5} (DONE)
        - Theres a chance that both the acres and lot lxw is added. Create a seperate regex pattern for that (DONE)
        - The current patterns returns whatever pattern is matches first. This creates a problem for results with AC and ACRES. How do I fix this? (DONE)
        ValueError: could not convert string to float: '175111.2RES': Error while type casting for column 'LOTSIZE (SQFT)' (DONE)
        - Lot sizes with just decimals numbers will trigger most of the patterns. How do I stop that? (DONE)
        - The patterns 'ACRE', 'ACS', 'Acres' doesnt show up in the acres_pattern. Add this
            - Turn the acres pattern into an re.compile object to include the re.ignorecase flag
        - Create a pattern for lot size when two or more pattern type values are found in the cells

- Cast the respective columns which use .str methods to strings in the clean_and_transfer_data functions
- Properly accept the property_archive() traceback and use the correct except block (DONE)
- Make sure the address regex patterns can recognize the S,W,E,N directions
- Integrate find_sq_ft for MUL properties (DONE)
    - Integrate the property type into sq_ft_search and sq_ft_keyerror (NOT NECESSARY)
    - Create two modular functions which should be housed inside sq_ft_search that will be activated depending on the property type (NOT NECESSARY)
    - The MUL function should be exactly identical to the RES function. Just replace 'SQFTAPPROX' with 'SQFTBLDG' (NOT NECESSARY)
    - Change the 'SQFTBLDG' column to 'SQFTAPPROX' during the cleaning so I dont have to unnecessarily create new functions (DONE)
- Create a function to house the string replacement methods on the address column
- Add logic that creates a new window when doing an rpr_search() (DONE)
    - Add logic to close the rpr_sq_ft window and clear the text from the rpr search box when search is complete then switch window (DONE)
- Work on potential_farm_area() (DONE)
    - How can I do this process easier?
- Separate property_archive function into smaller functions:
    - address_list_scrape() (DONE)
        - I need to log the address that I find as well to see the address format
- Refactor the property_archive() function to produce the property listing history (DONE)
- Add a function which looks into RPR if none of the property archive addresses work (DONE)
    - Make sure rpr works properly when an address isnt found
        - rpr_search_address(): Switch back to GSMLS tab if no address if found (Dont switch if theres no new tab open) (DONE)
        - I also need to clear the results of the search page too regardless of if theres anything found (DONE)
    - Make sure rpr_property_facts works as intended
- Figure out how to automate the operation of the macro immediately after download and dispose of the old files
    - Make sure the clean_db decorator skips the Batch File Conversion xlsx
- Create a new directory called 'GSMLS' in the Selenium Temp Folder to put downloads in. Switch to this directory in the main function
- Consolidate the 'MUL' and 'LND' quarterly_sales function into one. Accept property_type as an arg to use as a switch case
    - Modify the decorator to insert the property type as an arg
- Create a try-except block in the main function which can be used to restart the program recursively
- Refactor the main() function to not login to GSMLS if theres not data to download
- Use Funct.tools to get the proper names for the function when going through the decorators
- A selenium TimeoutError can occur if a page element isnt found. How to I get around this to continue the program?
_ Refactor the run_main decorator to run on a quarterly basis
    - uses a shelve file to know which dates to download for
- Fix the clean_and_transform function:
    - Round the BATHSTOTAL column to one dec place
    - SPLP should be rounded to 3 decimal places
- *** Create function to store all qtrly results in SQL after cleaning the data (DONE)
    - Update the rename_pandas_columns function to work for RES, MUL, and LND dbs. Also get the correct list of column names
    - Use the clean_db() and new pandas2sql() (DONE)
- Figure out how to properly calculate the Z-Score for the LND and MUL data
    - I may have to wait until the LND and MUL database is created then pull the mean prices from there
- Track rental prices as well

3) Complete lat_long function in GSMLS
    ****** Maybe shift to the GSMLS to use inside DealAnalyzer modules. (DONE)
    Only run function when analyzing distance instead of running the risk of getting banded for webscrapping
    ***The lat_long results should be immediately put back into SQL as a table update
    - The function will take the file as an arg and convert into Pandas db
    - Add Request module exceptions to the function
    - add logic for if county and or city vars arent None. Could be str or list types
    - Be sure to throttle the program by 1 sec after every request (DONE)
        - Build in a function to detect 400 and 500 level responses and adjust after
            - Use a While loop to continue running that address if 400 or 500 level responses are given
    - Delete all of the Selenium code and replace with Request and BS4 module syntax (DONE)
    - Track the 'importance' key of the JSON file. May depict the accuracy of the location (DONE)
- Run descriptive statistics for all cities: max, min, mean, median, mode, stddev and quartiles
- Create function which creates email campaigns for specific counties and cities based on analysis results (adjusted every qtr)
- *** Create a script that is solely run to download the quarterly sales results for RES, MUL, and LND properties
- *** Create a script that is solely run to clean and transform the data to be stored in SQL databases

4) Complete MachineLearning script
5) Start the DealAnalysis script
- Continually check if MLS deals are still available. Notice status changes
- Composite class (will use NJTaxAssessment, Foreclosures, GSMLS and MachineLearning
6) Complete HomeDepot Scrapper and Analysis
7) Complete Foreclosures script
8) NJ Planning Board Scraper
- Can I set up a REST API to check when each board is updated to then scrape?

NJTaxAssessment
________________________________________________________________________________
17) Add logger messages to necessary parts of the NJTaxAssessment code
20) Stream the zip file instead of downloading to reduce the built-in latency
    - Use the Requests ans Session.Requests module to stream the files. (ie: Scraper class) (PARTIALLY DONE)
    - Update the stream_zipfile function args to download_param= None, payload= None (is this still necessary?)
        - do if statement blocks that are executed based on the arg provided to the function
    - Needs to be implemented for the Essex County function (STREAMING EACH FILE CURRENTLY CANT BE DONE)
    *****There's a download limit on the website. Switch code to download all municipalities at one time
        - Build a Selenium latency to wait for download to finish (DONE)
        - Find a way to match the city with the downloaded file (NOT NECESSARY)
        **** Cookies wont let me download the file more than once. Download limit
            - Build in a timestamp method essex county scrape and timestamp= None arg into unzip and extract (DONE)
            If timestamp not None its automatically an Essex county file (DONE)
            If the delta between the timestamp and file download is less than X sec then name and move file, else continue
            - Can Python show metadata of a file? (DONE)
***21) Refactor city_database to open the "All Municipalities" xlsx for ESSEX county cities which dont have dbs
    - HOLD OFF ON THIS AS LAST RESORT
    - This code could present future errors for counties which have duplicate city spellings. Fix this
    - This file will only be used for cities which dont have DBs
5) Continue working on the long_lat function
    - Change the filename variable syntax so the files can be found
    - Create a db cleaning function to use inside lat_long
    - Change the sheet name during the db save to overwrite the first sheet

7)Foreclosure
____________________________________________________________________________________________________
    *** I want to store this as a SQL DB. This script will automatically check the site every week to update the status of the properties in the
    db. After X amount of time after the sale of the property, the SQL DB will delete that entry. And add new ones if they already dont exist
16) Alter the NJTaxAssessment class for the nj_databases to find the db for one county and one city as well
    - Create an arg entry for 'county' and 'city' in nj_databases
    - Create an arg entry for 'county' and 'city' in all_county_scrape
    - Create an arg entry for 'city' in essex_county_scrape
    ****Allow for str or list objects in these functions and alter the logic based on the object
    - Add 'if county is None, elif county is not None and city is None, elif county is not None and city is not None' in
        - all_county_scrape
        - nj_databases
    - Add 'if city is None, elif city is not None
18) Allow code to check what was downloaded already and start at new point
8) Continue working on the auction_locations function
    - Save this as a class variable
_______________________________________________________________________________________________________________________
Unassigned
3) Continue working on the property_tax_legend function
    - Create an excel sheet which will hold all the data
    - Use Regex to read the values of the cell and produce the values for the new column
9) Continue working on the haversine function
10) This is a program which will require the use of an SQL database
    Create a method which will transfer all of this data to PostgreSQL
11) Create a function which accepts the county and city as args, finds the latest downloaded zip (DONE)
    extracts the file and saves it as the city name in the county directory (DONE)
    11a) This should be a threaded operation. Create an empty list at the top of nj_database to store threaded operations (DONE)
14) Update waiting function
15) Update run_main decorator