PRIORITIES:
1) NJTaxAssessment
- Create a class called UniversalFunctions and add get_us_pw and buy_proxies and get_proxy to the list (DONE)
***Figure out how to properly add a package to sys.path and use it. Currently copied class from parent folder
- Download SeleniumWire
    - https://pypi.org/project/selenium-wire/ (DONE)
    - Add proxies to Selenium options (DONE)
- Get free/premium proxies to use in functions (DONE)
    - https://www.techradar.com/best/best-free-proxies (DONE)
- Create get_proxy function and use in all_counties and essex_county scrape
    - Create a decorator which extends its functionality to check if the proxies are still good. If not, buy new ones (DONE)
- Update city_database function
- Complete the code to stream Essex County property dbs (DONE)
    - The download_link isnt present sometimes and presents an error
    - Generate a different proxy then load webpage using city id (?town=0711), dont click the radio buttons anymore
2) GSMLS
- Complete the code to stream ALL property quarterly sales
- Repeat code for MUL and LND
- Run descriptive statistics for all cities: max, min, mean, median, mode, stddev and quartiles
3) Complete lat_long function in NJTaxAssessment
4) Complete MachineLearning script
5) Start the DealAnalysis script
- Continually check if MLS deals are still available. Notice status changes
- Composite class (will use NJTaxAssessment, Foreclosures, GSMLS and MachineLearning
6) Complete HomeDepot Scrapper and Analysis
7) Complete Foreclosures script
8) NJ Planning Board Scraper
- Can I set up a REST API to check when each board is updated to then scrape?
NJTaxAssessment
________________________________________________________________________________
17) Add logger messages to necessary parts of the NJTaxAssessment code
20) Stream the zip file instead of downloading to reduce the built-in latency
    - Use the Requests ans Session.Requests module to stream the files. (ie: Scraper class) (PARTIALLY DONE)
    - Update the stream_zipfile function args to download_param= None, payload= None (is this still necessary?)
        - do if statement blocks that are executed based on the arg provided to the function
    - Needs to be implemented for the Essex County function (STREAMING EACH FILE CURRENTLY CANT BE DONE)
    *****There's a download limit on the website. Switch code to download all municipalities at one time
        - Build a Selenium latency to wait for download to finish (DONE)
        - Find a way to match the city with the downloaded file (NOT NECESSARY)
        **** Cookies wont let me download the file more than once. Download limit
            - Build in a timestamp method essex county scrape and timestamp= None arg into unzip and extract (DONE)
            If timestamp not None its automatically an Essex county file (DONE)
            If the delta between the timestamp and file download is less than X sec then name and move file, else continue
            - Can Python show metadata of a file? (DONE)
***21) Refactor city_database to open the "All Municipalities" xlsx for ESSEX county cities which dont have dbs
    - HOLD OFF ON THIS AS LAST RESORT
    - This code could present future errors for counties which have duplicate city spellings. Fix this
    - This file will only be used for cities which dont have DBs
23) create decorator for nj_databases to create a time constraint for downloads (1 year)
24) Refactor database_check for ESSEX County
25) Start looking into using Proxies when using Selenium and Requests modules
    - Create function called get_proxy to return a random proxy to use with Selenium and Requests
    - Use SeleniumWire to use proxies with Selenium
    - from seleniumwire import webdriver as sw_webdriver
5) Continue working on the long_lat function
    - Change the filename variable syntax so the files can be found
    - Change the sheet name during the db save to overwrite the first sheet
    - Move the latitude and longitude columns up
        - df.insert(3, 'Latitude', df.pop('Latitude'))
        - df.insert(4, 'Longitude', df.pop('Longitude'))
    - Find the addresses which have the latitude and longitude = 0
    - Create a property address list that I can run through a for-loop
    - for i in property_address_list:
        city_db.at[i, 'Latitude'] =

7)Foreclosure
____________________________________________________________________________________________________
    *** I want to store this as a SQL DB. This script will automatically check the site every week to update the status of the properties in the
    db. After X amount of time after the sale of the property, the SQL DB will delete that entry. And add new ones if they already dont exist
16) Alter the NJTaxAssessment class for the nj_databases to find the db for one county and one city as well
    - Create an arg entry for 'county' and 'city' in nj_databases
    - Create an arg entry for 'county' and 'city' in all_county_scrape
    - Create an arg entry for 'city' in essex_county_scrape
    ****Allow for str or list objects in these functions and alter the logic based on the object
    - Add 'if county is None, elif county is not None and city is None, elif county is not None and city is not None' in
        - all_county_scrape
        - nj_databases
    - Add 'if city is None, elif city is not None
18) Allow code to check what was downloaded already and start at new point
8) Continue working on the auction_locations function
    - Save this as a class variable

GSMLS class
________________________________________________________________________________________________________________________
    - Create a function which finds the recently downloaded file and move it to a specific folder, Use NJTaxAssessment function as a template
    - For quarterly sales_res
        Create function for:
        - no county name(s) or city(s) provided (DONE)
        - one county name no city name
        - list of county names no city names
        - list of county names and list of names
    - Stream the xlsx file instead of downloading to reduce the built-in latency
        - Use the Requests an Session.Requests module to stream the files. (ie: Scraper class)
        - On the final download page, use the Elements and Network tabs to fill out the payload to stream the file
    - Refactor the res_property_styles to property_styles
        - Add property_type as a function arg and add if statement blocks to allow the use of RES, MUL or LND property types
_______________________________________________________________________________________________________________________
Unassigned
3) Continue working on the property_tax_legend function
    - Create an excel sheet which will hold all the data
    - Use Regex to read the values of the cell and produce the values for the new column
9) Continue working on the haversine function
10) This is a program which will require the use of an SQL database
    Create a method which will transfer all of this data to PostgreSQL
11) Create a function which accepts the county and city as args, finds the latest downloaded zip (DONE)
    extracts the file and saves it as the city name in the county directory (DONE)
    11a) This should be a threaded operation. Create an empty list at the top of nj_database to store threaded operations (DONE)
14) Update waiting function
15) Update run_main decorator

Use when you want to implement threading
ALL COUNTYS
# download_link1 = driver_var.find_element(By.XPATH, "/html/body/form/b[2]/big/a")
# download_link1.click()  # Step 5: Download the zip file
# threadobj = threading.Thread(target=NJTaxAssessment.unzip_and_extract,
#                              args=[counties[key], cities[k], download_link.split('/')[2]])  # Step 6: Unzip and save file
# started_threads.append(threadobj)
# threadobj.start()

ESSEX COUNTY
# threadobj = threading.Thread(target=NJTaxAssessment.unzip_and_extract,
#                              args=['ESSEX', cities[k]])  # Step 6: Unzip and save file
# started_threads.append(threadobj)
# threadobj.start()
